<pre class='metadata'>
Title: Ethical Principles for Web Machine Learning
Shortname: ethical-webmachinelearning
Level: none
Status: ED
Group: webmlwg
Repository: webmachinelearning/ethical-webmachinelearning
TR: https://w3.org/TR/ethical-webmachinelearning
URL: https://webmachinelearning.github.io/ethical-webmachinelearning
Editor: James Fletcher, BBC, https://bbc.co.uk
Editor: Anssi Kostiainen 41974, Intel Corporation, https://intel.com/
Boilerplate: conformance no, index no, issues-index no
Abstract: This document discusses ethical issues associated with using Machine Learning and outlines considerations for web technologies that enable related use cases. The whole document is open for comment and review, but input is particularly sought on Sections 3, 4 and 5. 
Status Text: This is a document to enable consultation and stakeholder input into the Web Machine Learning Working Group’s Note on Ethical Principles for Web Machine Learning.  This document is aimed to be published by the [Machine Learning Working Group](https://www.w3.org/groups/wg/webmachinelearning) as a Working Group Note and does not contain any normative content. This document is for guidance only and does not constitute legal or professional advice. The document will evolve and receives updates as often as needed.
Markup Shorthands: markdown yes
</pre>
<pre class=biblio>
{
 "UNESCO": {
  "href": "https://unesdoc.unesco.org/ark:/48223/pf0000380455",
  "title": "Recommendation on the Ethics of Artificial Intelligence",
  "publisher": "UNESCO"
},
  "FAST": {
    "href": "https://w3c.github.io/apa/fast/checklist.html",
    "title": "Framework for Accessibility in the Specification of Technologies",
    "publisher": "W3C Accessible Platform Architectures Working Group"
},
  "CDEI": {
     "href":
     "https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/938857/Summary_Slide_Deck_-_CDEI_review_into_bias_in_algorithmic_decision-making.pdf",
     "title": "Review into Bias in Algorithmic Decision-Making",
     "publisher": "Centre for Data Ethics and Innovation (2020)"
   },
   "Mehrabi": {
     "href": "https://arxiv.org/pdf/1908.09635.pdf",
     "title": "A Survey on Bias and Fairness in Machine Learning",
     "authors": ["Ninareh Mehrabi", "Fred Morstatter", "Nripsuta Saxena",
"Kristina Lerman", "Aram Galstyan"]
   },
   "Leslie": {
     "authors": ["Leslie D."],
      "href": "https://www.turing.ac.uk/sites/default/files/2019-08/understanding_artificial_intelligence_ethics_and_safety.pdf",
      "title": "Understanding artificial intelligence ethics and safety",
      "publisher": "The Alan Turing Institute (2019)"
   },
   "Treviranus": {
      "authors": ["Jutta Treviranus"],
      "href": "https://www.w3.org/2020/06/machine-learning-workshop/talks/we_count_fair_treatment_disability_and_machine_learning.html",
      "title": "We Count: Fair Treatment, Disability and Machine Learning"
},
"Bourtoule": {
      "authors": ["Bourtoule, L. et al"],
      "title": "Machine Unlearning",
      "href": "https://arxiv.org/pdf/1912.03817.pdf"
},
"Crawford": {
      "authors": ["Crawford, K"],
      "publisher": "Conference on Neural Information Processing Systems, invited speaker.",
      "title": "The trouble with bias",
      "href": "https://www.youtube.com/watch?v=fMym_BKWQzk"
},
"FATML": {
  "title": "Principles for Accountable Algorithms",
  "href": "https://www.fatml.org/resources/principles-for-accountable-algorithms"
},
"EGTAI": {
  "title": "Ethics Guidelines for Trustworthy AI",
  "href": "https://www.aepd.es/sites/default/files/2019-12/ai-ethics-guidelines.pdf",
  "publisher": "EU High Level Expert Group on Artificial Intelligence (2018)"
},
"AI4People": {
  "authors": ["Floridi et al"],
  "title": "AI4People - An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations",
  "href": "https://link.springer.com/article/10.1007/s11023-018-9482-5"
},
"Gall": {
  "authors": ["Gall, R."],
  "title": "Machine Learning Explainability vs Interpretability: Two concepts that could help restore trust in AI",
  "href": "https://www.kdnuggets.com/2018/12/machine-learning-explainability-interpretability-ai.html#:~:text=Interpretability%20is%20about%20the%20extent,be%20observed%20within%20a%20system.&text=Explainability%2C%20meanwhile%2C%20is%20the%20extent,be%20explained%20in%20human%20terms."
},
"Rodrigues": {
  "authors": ["Rodrigues, R", "Tesseguier, R."],
  "title": "The underdog in the AI ethical and legal debate: human autonomy",
  "href": "https://www.ethicsdialogues.eu/2019/06/12/the-underdog-in-the-ai-ethical-and-legal-debate-human-autonomy/#:~:text=It%20describes%20a%20person's%20ability,to%20shape%20their%20own%20lives."
},
"Smuha": {
  "authors": ["Smuha, N."],
  "href": "https://policyreview.info/pdf/policyreview-2021-3-1574.pdf",
  "title": "Beyond the individual: governing AI’s societal harm"
},
"Suresh": {
  "authors": ["Suresh"],
  "title": "A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle",
  "href": "https://arxiv.org/pdf/1901.10002.pdf"
},
"Vallor": {
  "authors": ["Vallor", "Green", "Raicu"],
  "title": "Overview of Ethics in Tech Practice",
  "href": "https://www.scu.edu/ethics-in-technology-practice/overview-of-ethics-in-tech-prac"
},
"Zeng": {
  "authors": ["Yi Zeng", "Enmeng Lu", "Cunqing Huangfu"],
  "title": "Linking AI Principles",
  "href": "https://www.linking-ai-principles.org/principles"
},
"Weidinger": {
  "authors": ["Weidinger et al"],
  "title": "Ethical and social risks of harm from Language Models",
  "href": "https://arxiv.org/pdf/2112.04359.pdf"
},
"Vaughan": {
  "authors": ["Wortman", "Vaughan", "Wallach"],
  "title": "A Human-Centered Agenda for Intelligible Machine Learning",
  "href": "http://www.jennwv.com/papers/intel-chapter.pdf"
},
"Xue": {
  "authors": ["M. Xue", "C. Yuan", "H. Wu", "Y. Zhang", "W. Liu"],
  "title": "Machine Learning Security: Threats, Countermeasures, and Evaluations",
  "href": "https://ieeexplore.ieee.org/document/9064510"
},
"Khan": {
  "authors": ["Arif Ali Khan", "Sher Badshah", "Peng Liang", "Bilal Khan", "Muhammad Waseem", "Mahmood Niazi", "Muhammad Azeem Akbar"],
  "title": "Ethics of AI: A Systematic Literature Review of Principles and Challenges",
  "href": "https://arxiv.org/pdf/2109.07906.pdf"
},
"Floridi": {
  "authors": ["Luciano Floridi", "Josh Cowls"],
  "href": "https://hdsr.mitpress.mit.edu/pub/l0jsh9d1/release/7",
  "title": "A Unified Framework of Five Principles for AI in Society "
},
"Jobin": {
  "authors": ["Anna Jobin", "Marcello Ienca", "Effy Vayena"],
  "title": "The global landscape of AI ethics guidelines",
  "href": "https://www.nature.com/articles/s42256-019-0088-2"
},
"What2How": {
  "authors": ["Jessica Morley", "Luciano Floridi", "Libby Kinsey", "Anat Elhalal" ],
  "title": "From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices",
  "href": "https://link.springer.com/article/10.1007/s11948-019-00165-5"
}
}
</pre>

# Introduction

<blockquote>That AI will have a major impact on society is no longer in question. Current debate turns instead on how far this impact will be positive or negative, for whom, in which ways, in which places, and on what timescale. [[AI4People]]</blockquote>


<blockquote>There is no 'silver bullet' here; creating technologies that will promote human flourishing and sustainable life on this planet is hard and uncertain work, involving difficult tradeoffs, some inevitable failures, and challenges that defy simple and stable solutions. But it is good work, work that can and must be done. [[Vallor]]</blockquote>



Machine Learning (ML) is a powerful technology, whose application to the web promises to bring benefits and enable compelling new user experiences. But there is also increasing awareness that ML applications can create harms, intentional and unintentional, that impact individual users, communities and society. 

W3C’s mission is to “ensure the long-term growth of the web” and this is best achieved where the potential harms of new technologies like ML are considered and mitigated through a comprehensive ethical approach to the design and implementation of Web ML specifications. 

As required by the charter of the [Web Machine Learning Working Group](https://www.w3.org/groups/wg/webmachinelearning), this document sets out such an ethical approach. It contains a set of ethical principles and guidance. It includes some general consideration of harms, risks and mitigations relevant to Web ML. And it offers a practical process for supplementing those general considerations with concrete risks and mitigations for specific use cases.

NOTE: In broader debate, the terms Artificial Intelligence and Machine Learning, and their related ethical considerations (AI/ML Ethics) are often used interchangeably. Given the focus of the WG on Machine Learning, this document will generally use the terms Machine Learning or ML, and Machine Learning or ML Ethics, with the intent to refer to the broader set of issues and concerns encompassed by AI/ML.


# Machine Learning on the Web

In parallel to general advances in ML, the web platform is gaining [client-side Machine Learning capabilities](https://github.com/webmachinelearning/webnn/blob/main/explainer.md). Currently machine learning inference in the browser uses the WebGL graphics API, but the lack of access to platform capabilities beneficial for ML such as dedicated ML hardware accelerators constrains the scope of experiences and leads to inefficient implementations on modern hardware. 

The [Web Machine Learning Working Group](https://www.w3.org/groups/wg/webmachinelearning) aims to develop standards to enable access to these client-side capabilities. In web-based ML applications, the model may reside on the server or on the client, and the data processing, or inference, can be offloaded to the client.

[Example use cases](https://webmachinelearning.github.io/webnn/#usecases) include person detection, facial recognition, image captioning, machine translation and noise suppression.

Web machine learning has a number of potential benefits. It could make large-scale deployment of ML systems feasible without investment in cloud-based infrastructure. This opens the door to tens of millions of do-it-yourself web developers and aligns this technology with the [decentralized web architecture](https://w3ctag.github.io/ethical-web-principles/#control) ideal that minimizes single points of failure and single points of control.

Local processing could also enable machine learning use cases that require low latency, such as object detection in immersive web experiences. By offloading computationally expensive tasks involving ML to on-device hardware, any web application could be enriched with ML capabilities, and existing web content progressively enhanced. 

With appropriate safeguards, enabling machine learning inference in the browser (as opposed to in the cloud) *could* also enhance privacy, since input data such as locally sourced images or video streams stay within the browser's sandbox. 

# General ethical issues in Machine Learning

As well as potential benefits, there is increasing awareness that the application of machine learning poses risks and can lead to harms, raising a range ethical questions. This section presents a brief overview of some key concerns.

For a general background on ethics and its relevance to ML, see [[#appendix-background]].


## Accuracy

The accuracy of an ML model is the proportion of examples for which it generates a correct output [[Leslie]]. In general high accuracy is a good thing, and low accuracy can lead to harms, for example where facial recognition systems are used in law enforcement. But highly accurate facial recognition systems can also pose risks to privacy and autonomy (e.g. mass surveillance). 

In some areas such as credit-scoring or loan approval, increasing the accuracy of predictions might come at the cost of requiring access to too much personal data. 

There is also concern about the [over-hyping of the ability of AI to accurately predict certain things at all](https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf), particularly social outcomes such as job performance or criminal recidivism. Accuracy may be a useful measure where an area has a clear, objective ground truth (e.g. vehicle license-plate recognition) but many areas of human judgment are nuanced, messy and contextual, and simple accuracy risks being too reductive a measure.


## Bias

Bias has a number of meanings, including a systematic deviation from a true value. This can be positive or negative. Bias is a prominent concern in ML ethics, where the concern more specifically is ‘a systematic skew in decision-making that results in unfair outcomes’ [[CDEI]].

Concerns about bias are particularly prominent where negative outcomes (such as inaccurate predictions and their consequences) disproportionately affect individuals or groups who are vulnerable or historically marginalised. Where the unfair treatment relates to protected characteristics such as race, gender, disability or sexuality, bias can constitute illegal discrimination, depending on relevant laws. 

There are a number of causes of bias [[Mehrabi]], ranging from issues with data to algorithmic design and human perception and decision-making. Perhaps the most prominent cause is that algorithms trained to make decisions based on past data will often replicate the historic biases in that data ([[Suresh]] also has a useful survey of causes of bias).



## Fairness

There is [no single definition of fairness](https://www.youtube.com/watch?v=jIXIuYdnyyk&ab_channel=ArvindNarayanan) ([[Mehrabi]] also has a good survey of definitions) - like ethics it is contextual and varies according to different values, perspectives and societies. But one core idea is that people should be treated equally unless there is a justified, relevant reason not to.

Fairness is often a lens through which to make sense of other ethical concerns. As noted above, bias can be positive or negative - it’s when it leads to ‘unfair’ outcomes that it is problematic. Where ethical principles or concerns need to be balanced against each other, considering fairness often provides a guide to how to do that. 

Fairness is about both outcomes and process. Outcomes should involve the fair distribution of benefits and costs, and the avoidance of unfair bias or arbitrary decisions. Procedural elements of fairness include involving communities that will be affected by ML outputs in decisions about how the systems are designed and used, and ensuring there is the ability to contest and seek redress for decisions made by ML.

Fairness could also arguably justify bias - for example biasing a system to favour people who have been historically marginalised, in order to achieve an outcome which is in some sense equal or fair (this is known as equity - treating people differently on the basis of need to achieve outcomes which are fair).

Another important aspect of fairness is the distribution of access to computationally complex ML approaches, and the benefits that come from access. People living in countries with less powerful or functioning infrastructure, or who cannot access sufficient computing power, may be unfairly disadvantaged. 


## Safety & Security

Safety includes that an ML system should be accurate, but also that it should be reliable (perform as intended, and continue to do so over time), secure (against adversarial attacks), and robust enough to do these things in real-world, unpredictable and sometimes challenging conditions ([[Leslie]])

Safety is a broad concern, but is particularly relevant where the failure of ML systems could result in real-world harm - for example with medical diagnosis or self-driving cars. 

There are a number of security risks to machine learning, including training data poisoning, adversarial inputs, or model inversion and adversarial inference attacks which can expose model parameters or training data ([[Xue]]).

Machine learning can also increase the effectiveness of other types of security attacks, for example by enabling more effective impersonation for social engineering and phishing attacks.


## Privacy

There are a number of ways in which ML systems can pose risks to privacy. 

One is where systems that undermine privacy operate without a user’s knowledge or explicit, informed consent. This is true of systems that undermine privacy explicitly (surveillance systems), but also where undermining privacy is a potential byproduct of intended, legitimate use (e.g. if an ML system which has access to a user’s video camera).

There are also privacy concerns about the data used to train models. Data may be collected in a way which violates privacy, such as without consent from users (e.g. scraping personal information). Models may ‘leak’ personal data (e.g. large language models [[Weidinger]]). Legitimately collected data may also be compromised, for example through reverse engineering or inference style attacks which can de-anonymise model training data.

The accuracy of the predictions of ML systems may also present risks. Just as the outputs of sensor APIs could be used to identify, fingerprint or correlate user activity (e.g. if the output is too precise), it is possible that the outputs of ML systems could pose similar risks.

And use of ML systems to infer sensitive, personal data about users based on non-sensitive data (e.g. inferring sexuality from content preferences) may also violate privacy. 

Some jurisdictions (e.g. EU/GDPR) also provide a ‘right to be forgotten’, which arguably could include being removed from ML training data. So a privacy-protecting approach would need to ensure that appropriate processes and technical capabilities are in place for this to happen (see e.g. [[Bourtoule]]).


## Transparency

Very broadly, transparency is about users and stakeholders having access to the information they need to make informed decisions about ML. It’s a holistic concept, covering both ML models themselves and the process or pipeline by which they go from inception to use. [[Vaughan]] (following the [[EGTAI]]) propose 3 key components:

- **Traceability**: Those who develop or deploy machine learning systems should clearly document their goals, definitions, design choices, and assumptions.
- **Communication**: Those who develop or deploy machine learning systems should be open about the ways they use machine learning technology and about its limitations.
- **Intelligibility**: Stakeholders of machine learning systems should be able to understand and monitor the behavior of those systems to the extent necessary to achieve their goals.

Understanding ML systems involves two key related concepts [[Gall]]:

- **Interpretability**: is about the extent to which a cause and effect can be observed within a system. 
- **Explainability**: the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms.

Lack of interpretability and explainability is known as the black-box problem, which is particularly prevalent with more complex ML approaches such as neural networks. 


## Accountability

Given that ML systems are increasingly being used in high impact areas (healthcare, welfare, criminal justice) and that harms can be large when they go wrong, and that actors in the ML pipeline take responsibility for considering the impact of ML systems, and accountability for when things go wrong.

“Algorithms and the data that drive them are designed and created by people – there is always a human ultimately responsible for decisions made or informed by an algorithm. "The algorithm did it" is not an acceptable excuse if algorithmic systems make mistakes or have undesired consequences, including from machine-learning processes.” [[FATML]]

Transparency is an enabler for accountability (we need to be able to see what is going wrong and where to be able to determine responsibility). It also requires proper processes for the consideration of risks to be in place, documentation of policies and processes, and the means for those who are harmed to seek redress. The developers of ML systems should also take responsibility for any 3rd party ML they use in their system. 

Increasingly in some jurisdictions, there are formal legal mechanisms for accountability and seeking redress.


## Human Control and Decision-making

The need for accountability, as well as other concerns above such as accuracy and fairness,  have led to the assertion of the importance of humans making in the final decision in high stakes applications. More broadly, ML applications should always be under ultimate human control. 

But there are pitfalls too where ML approaches support human decision-making - problems with explainability can inhibit the full exercise of human capabilities, or humans may exhibit “automation bias” where they place too much trust in information or recommendations provided by an ML system.


## Environmental Impact & Sustainability

There is increasing awareness that computationally complex ML approaches trained on very large data sets can have a large environmental impact, given the amount of energy required to power the training phase. 

The broader concern with sustainability suggests that ML applications and systems should not undermine the sustainability of the physical, social and political ecosystems in which they’re deployed. This might include the impact on jobs, employment and the economy, or on the quality of and access to information necessary for a functioning democratic system.


## Types of harm

The above list contains some potential sources or causes of harm from machine learning. It is also important to be aware that harm can take a number of different forms, all of which should be considered. 

As noted above, harms can impact individuals, groups and society. To take the example of a biased facial recognition system [[Smuha]]:

- this may lead to wrongful discrimination against an **individual** (e.g. wrongful arrest).
- where a number of individuals who belong to a **group or collective** suffer this discimination (e.g. because of shared ethnicity), there is a group harm. This could be the sum of the individual harms, as well as harms such as an increase in prejudice towards that group caused by the perpetuation of historic bias. 
- here could be a harm to the interests of **society**, such as being able to ‘live in a society that does not discriminate against people based on their skin colour and that treats its citizens equally.’ [[Smuha]]

Harms can also take a number of forms. These can include:

- **physical**, either directly (e.g. the failure of driver-less cars), or indirectly (e.g. flaws in a system leading to incorrect medical diagnosis). 
- **allocative**, when a system unfairly allocates or withholds from certain individuals or groups an opportunity or a resource (e.g. benefits or loans) [[Crawford]].
- **representational,** when systems “reinforce the subordination of some groups along the lines of identity.” [[Crawford]] e.g. when a Google search for ‘CEO’ returns mostly pictures of white men, or image recognition systems generate offensive labels for people of colour.

# Ethical Principles for Web ML

The following ethical values and principles are taken from the UNESCO [Recommendation on the Ethics of Artificial Intelligence](https://unesdoc.unesco.org/ark:/48223/pf0000380455) [[UNESCO]]. They were developed through a global, multi-stakeholder process, and have been ratified by 193 countries. An additional principle of ‘Autonomy’ has been added, and the principles have been re-ordered. For more on why these have been adopted, see [[#appendix-unesco]].

These values and principles should drive the development, implementation and adoption of specifications for Web Machine Learning. They include guidance (adapted from UNESCO and W3C sources) which provides further detail on how the values and principles should be interpreted in the W3C web machine learning context. 

The following terms are used:

- ‘ML actors’ refers to stakeholders involved in web ML: specification writers, implementers and web developers
- ‘ML systems’ refers to the ML model or application that is making use of web ML capabilities

The next section (S.5) provides further guidance on how to operationalize the principles and turn them into specific risks and mitigations.


## UNESCO Values

These indicate desirable behavior and represent the foundation of the principles


<h4 class=no-num>VALUE 1) Respect, protection and promotion of human rights and fundamental freedoms and human dignity</h4>



ML actors and systems should treat all human beings as being of equal worth, and no individual, group or society should be harmed. ML systems should be designed in a human-centric way, to promote human flourishing, and should respect and enhance human autonomy. This includes enabling meaningful agency, control and choice.

ML actors should not enable state censorship, surveillance or other practices that seek to limit these freedoms. Nor should they enable manipulation, misinformation, harassment or persecution. 


<h4 class=no-num>VALUE 2) Environment and ecosystem flourishing</h4>

Environmental and ecosystem flourishing should be recognized, protected and promoted through the life cycle of ML systems. ML actors should reduce the environmental impact of ML systems,  including  but  not  limited  to  their  carbon footprint,  to  ensure  the  minimization  of  climate  change  and environmental  risk  factors,  and  prevent  the  unsustainable  exploitation,  use  and  transformation  of  natural resources contributing to the deterioration of the environment and the degradation of ecosystems.


<h4 class=no-num>VALUE 3) Ensuring diversity and inclusiveness</h4>

Respect, protection and promotion of diversity and inclusiveness should be ensured throughout the life cycle of ML systems. This may be done by enabling the active participation of all individuals or groups, regardless of lifestyle choices, beliefs, opinions, expressions or personal  experiences.This should include the meaningful option not to use ML systems. ML users should not be disadvantaged because they lack necessary technological  infrastructure,  education  or  skills.

<h4 class=no-num>VALUE 4) Living in peaceful, just and interconnected societies</h4>

ML systems should not segregate, objectify or undermine freedom and autonomous decision-making or the safety of human beings and communities. They should not divide and turn individuals and groups against each other, or threaten coexistence between humans, other living beings and the natural environment. ML systems should be built to cross regional and national boundaries. 


## UNESCO Principles

These unpack the values underlying them more concretely so that the values can be more easily operationalized.


<h4 class=no-num>PRINCIPLE 1) Proportionality and Do No Harm</h4>

When developing ML systems, developers should consider what harm the application could do to individuals, groups and society, especially to vulnerable people. They should seek to eliminate or minimise those harms. 

The use of ML and any data gathered to enable it must be proportional to achieve a given legitimate aim. It should not violate human rights, or be used for social scoring or mass surveillance purposes.

ML actors should prioritize potential benefits for users over potential benefits to developers, content providers, user agents, advertisers or others in the ecosystem, in line with the priority of constituencies.

<h4 class=no-num>PRINCIPLE 2) Fairness and non-discrimination</h4>

The benefits of ML systems should be available and accessible to all.

ML actors should minimize and avoid reinforcing or perpetuating bias and discrimination, particularly against vulnerable and historically marginalised groups.They should ensure that the outcomes of ML applications are fair, and that effective remedy is available against discrimination and biased algorithmic determination.

This principle also applies to access to web ML systems: they should be fully accessible to people with disabilities, appropriately localized, and accommodate people on low bandwidth networks and with low specification equipment. 

For further guidance on accessibility, see  the draft checklist to support Framework for Accessibility in the Specification of Technologies [[FAST]].


<h4 class=no-num>PRINCIPLE 3) Autonomy</h4>

ML systems should respect and enhance human autonomy. This includes enabling meaningful agency, control and choice. Users should give informed consent before Web ML is used.

ML systems could be used to manipulate and deceive people, complicate isolation, and encourage addictive behaviors. ML actors should mitigate against these potential abuses and patterns when creating ML systems, and avoid introducing technologies that increase the chance of people being harmed in this way.


<h4 class=no-num>PRINCIPLE 4) Right to Privacy, and Data Protection</h4>

ML systems should be designed and implemented to ensure that privacy and personal information is protected throughout the life cycle of the application. ML actors ensure they are accountable for this, and should conduct adequate privacy impact assessments, and implement privacy by design approaches. 

Data should be collected, used, shared, archived and deleted in ways that are consistent with local and international law.

For further guidance on privacy, see [[security-privacy-questionnaire]].


<h4 class=no-num>PRINCIPLE 5) Safety and security</h4>

ML systems should actively support safety and security. Unwanted harms (safety risks), as well as vulnerabilities to attack (security risks) should be avoided and should be addressed, prevented and eliminated throughout the life cycle of ML systems to ensure human, environmental and ecosystem safety and security. ML actors should ensure that systems are accurate, reliable and robust over their entire life-cycle, and should make sure that users understand any risks they are taking when using an application.

See also [[security-privacy-questionnaire]].


<h4 class=no-num>PRINCIPLE 6) Transparency and explainability</h4>

ML actors and systems should support transparency and explainability. It should always be possible to determine how a web ML application was built and how the code works, in line with the "view source" ethos of the web. 

Users and third-parties such as civil society groups and researchers should be able to audit and inspect ML systems for security, privacy, bias, fairness and other ethical concerns outlined in these principles.

ML actors should inform users when a product or service is provided directly or with the assistance of ML, and users should be fully informed when a significant decision is informed by or is made on the basis of ML. They should be able to access the reasons for a decision affecting their rights and freedoms, and ML actors should provide means for users to request review and correction of these decisions.

ML systems and outcomes should be explainable. ML actors should promote tools and approaches that enhance explainability and meaningful user control.

ML actors should also be transparent about the steps they have taken to consider and implement these ethical principles. 


<h4 class=no-num>PRINCIPLE 7) Responsibility and accountability</h4>

Appropriate oversight, impact assessment, audit and due diligence mechanisms should be developed to ensure accountability for ML systems and their impact throughout their life cycle.

It should always be possible to attribute ethical responsibility and liability for the outcomes of ML systems, and decisions and actions based on them, to people or organisations corresponding to their role in the life cycle of the ML system.

ML actors should take responsibility for third-party ML models and approaches used in their systems.


<h4 class=no-num>PRINCIPLE 8) Sustainability</h4>

ML actors should ensure that the human, social, cultural, economic and environmental impact of ML applications is sustainable.

They should develop and favour approaches which minimise power consumption, data storage and processing requirements, and maxmimise the lifespan of physical devices through maintaining compatibility.


<h4 class=no-num>PRINCIPLE 9) Human oversight and determination</h4>

In scenarios where decisions are understood to have a high impact or one that is irreversible or difficult to reverse, or may involve life and death decisions, final human determination should apply. 

The decision by users to cede control and decision-making power to an ML system should be explicit, fully informed and limited to a specific context.


<h4 class=no-num>PRINCIPLE 10) Awareness and literacy</h4>

ML actors should ensure that all members of society have access to the information they need to make informed decisions about their use of ML systems. 


<h4 class=no-num>PRINCIPLE 11) Multi-stakeholder and adaptive governance and collaboration</h3></h4>

The participation of different stakeholders throughout the life cycle of ML projects is necessary to enable the benefits to be shared by all, and to mitigate risks.

Where tensions arise between the principles above, they should be resolved through democratic stakeholder engagement and participation.

Web ML specifications and systems should support open standards and interoperability.


# Operationalization: Putting the Principles into Practice (WIP)
This section will offer practical guidance on how to operationalize the values and principles above.

It will outline a process that ML actors can follow to use the principles to think through specific risks and mitigations relevant to their specific use case. 

It will also start to gather general, high-level risks and mitigations that can be a starting point and guide for operationalizing the principles.

<div class=note>The table below makes a start on this … Reviewers / commenters are also invited to add to the table from their own knowledge and expertise. This will be further supplemented with some group brainstorming sessions.</div>

<table class=data>
<thead>
<tr><th>Principle</th><th>Risks</th><th>Possible Mitigations</th></tr>
</thead>
<tbody>
<tr><th>Proportionality and Do No Harm</th><td></td><td></td>
<tr><th rowspan=2>Fairness and non-discrimination</th><td><p>Scaling up ML via browsers creates risks of scaling up bias issues linked to ML training.</p><p></p><p>ML approaches optimize for the majority, leaving minorities and underrepresented groups at risk of harm or sub-optimal service (see e.g. [[Treviranus]])</p><p></p><p></p><p></p></td><td>Browser-assisted mechanisms to find out about the limitations and performance characteristics of ML models used in a Web app. This would build on an approach published in Model Cards for Model Reporting where making this report machine-discoverable would allow for the web browser to offer a more integrated user experience.<td>
<tr><td>Differences in Internet connection speeds across geographical locations and large size of production-grade models means the user experience of on-device inference is not equal in all locations.</td><td>This issue is not specific to ML and can be mitigated in part by using a Content Delivery Network and by offering reduced size models. </td>
<tr><th>Autonomy</th><td></td><td>Similarly to videos, the sites should make it opt-in to load large models on load or run expensive compute tasks.</td>
<tr><th>Right to Privacy, and Data Protection</th><td></td><td>requiring explicit consent to access privacy-sensitive capabilities such as on-device camera.</td>
<tr><th>Safety and security</th><td></td><td></td>
<tr><th rowspan=2>Transparency and explainability</th><td></td><td><p>Web APIs by their design make it possible to integrate into browsers developer tools features that help build intuition on how neural networks work, in the spirit of "view source" principle.</p><p></p><p>Web-based visualization tools have been developed for deep networks for educational use and their integration into browsers remains further work.</p></td>
<tr><td></td><td>Integrate into web browser developer tools a conceptual graph of the model’s structure to inspect and understand the model architecture.</td>
<tr><th>Responsibility and accountability</th><td></td><td></td>
<tr><th>Sustainability</th><td>Web ML applications are compute / energy intensive, and widespread adoption exacerbates environmental problems.</td><td>Opportunity for web browsers to make visible the energy impact of various workloads running in the browser, for example through the proposed Compute Pressure API.</td>
<tr><th>Human oversight and determination</th><td></td><td></td>
<tr><th>Awareness and literacy</th><td></td><td></td>
<tr><th>Multi-stakeholder and adaptive governance and collaboration</th><td></td><td></td>
</tbody>
<table>

**What happens if principles are in conflict or tension with each other?** From UNESCO:

<blockquote>While all the values and principles … are desirable per se, in any practical contexts, there may be tensions between these values and principles. In any given situation, a contextual assessment will be necessary to manage potential tensions, taking into account the principle of proportionality and in compliance with human rights and fundamental freedoms … To navigate such scenarios judiciously will typically require engagement with a broad range of appropriate stakeholders, making use of social dialogue, as well as ethical deliberation, due diligence and impact assessment.</blockquote>



# Appendix 1. Background: Ethics & Machine Learning # {#appendix-background}

## What is ethics?

Ethics is about what is right and wrong, good and bad. All of us think about ethics  all the time as we think about what’s right and wrong and make decisions about how to act accordingly. 

Philosophical discussions around ethics seek to ground those thoughts about right and wrong in a rational context. They generally consider ethical issues at three levels, from the abstract to the concrete:

- **Meta-ethics** is the most abstract, concerned with questions like whether concepts of right and wrong are objective facts or subjective values
- **Normative ethics** is the more practical consideration of how we should act, both in terms of broad principles (e.g. treat others as you would want to be treated yourself) and more specific rules (e.g. do not steal)
- **Applied ethics** goes even further, considering how normative considerations should be applied in specific situations or domains, such as medical ethics, bio-ethics or AI ethics.

So ethical systems or frameworks are concerned with both broad principles to guide ethical thinking, and providing more specific answers to or guidance on a range of ethical questions.

The following is [a useful sense of what ethics is/isn’t from the Markkula Center for Applied Ethics](https://www.scu.edu/ethics-in-technology-practice/overview-of-ethics-in-tech-practice/)

Ethics isn’t​:

- Legal/Corporate 'Compliance' (Legal ≠ Ethical; Ethical ≠ Legal)
- A Set of Fixed Rules to Follow (No fixed set of rules can cover all ethical cases/contexts)
- A Purely Negative Frame: (“Don't do that! Or That! Or THAT!”)
- Subjective Sense of Right/Wrong ("You have your ethics, I have mine")
- Religious Belief ("It's right/wrong simply because my religion says so")
- Non-moral Customs of Etiquette ("That is just Not Done here")
- Uncritical Obedience to Authority (“Good Germans'/'Good Americans”)

Ethics ​Is​:

- Promoting objective (but context & culture-dependent) conditions of human flourishing
- Respecting the dignity of others and the duties created in our relationships to them
- Living as a person of integrity and principle
- Promoting beneficial and just outcomes, avoiding and minimizing harm to others
- Cultivating one's own character to become increasingly more noble and excellent
- A skillful practice of moral perception, sensitivity, and flexible, discerning judgment
- Learning to more expertly see and navigate the moral world and its features

The idea of ethics as a practice is important. Ethical principles are valuable, but by themselves achieve little - they are often abstract and not directly actionable. They must be turned into concrete outcomes to have an effect. Although there may be pre-existing approaches and best practices to draw on to do this, in a fast-moving area like ML it is often necessary to think through new ethical challenges to come up with appropriate solutions. 

Thankfully, applied ethics also concerns itself with the development of tools to support this type of thinking. In ML ethics, these tools help people facing ethical questions to work them through, moving from principles, to thinking about the impact of particular approaches or technologies, their benefits and potential risks and harms, and how those might be mitigated to ensure the overall ethical and beneficial impact of the approach.

This note will do the same - it will propose a set of ethical principles for Web ML, and offer guidance on how to turn those principles into practice.

For those interested to explore further:

- the Markkula Center has a [useful and comprehensive set of resources](https://www.scu.edu/ethics-in-technology-practice/) devoted to technology ethics and translating principles into practice.
- The University of Helsinki has a free MOOC on [Ethics of AI](https://ethics-of-ai.mooc.fi/) 
- Another good MOOC is the University of Edinburgh / EdX course [Data Ethics, AI and Responsible Innovation](https://www.edx.org/course/Data-Ethics-AI-and-Responsible-Innovation)


## 1.2 Why and how does ethics apply to machine learning?

It’s clear that ML can have a big impact on people’s lives and experience. So we could ask whether that impact is good or bad, and also how we might act to try to ensure that the impact is good rather than bad. Ethics could help us answer those questions.

Why should we take an ethical approach? Firstly, [it is a deliverable required in the working group’s charter](https://www.w3.org/2021/04/web-machine-learning-charter.html). 

But as ethics is the active consideration of what’s good and bad, rather than the uncritical acceptance of rules, it’s worth considering why it should be a deliverable. There are a number of reasons:

- Technology is never neutral - it will always have social and ethical implications. The question is whether these are actively considered and addressed, or not. 
- Given the scale and depth of the impact that AI/ML is anticipated to have, failure to consider the ethical implications could cause (and indeed is already causing) great harm. If technologies are not aligned with the values of the societies they operate in, they risk undermining them.
- There is clear demand for an ethical approach to ML, seen through activism from civil society, the emergence of &gt;100 sets of ethical AI principles globally, and government moves all around the world to regulate AI.
- It aligns with the [W3C’s mission and design principles](https://www.w3.org/Consortium/mission):
  - W3C’s mission is to “ensure the long-term growth of the web” - this is unlikely if web technologies create more harm than good
  - W3C design principles include “Web for all” and “Web of trust” which suggest that W3C’s approach to web standards is values-based
  - W3C TAG is developing [Ethical Web Principles](https://w3ctag.github.io/ethical-web-principles/). These are not normative, but provide a more explicit signal that W3C supports an ethical approach to web technologies.

## 1.3 The universality of the web vs the specificity of ethics

The web is a universal technology, used around the world by people of all different nationalities, races, religions and beliefs. 

By contrast, ethical systems are often specific to particular groups or societies - for example religions, professional groups or regions and countries. 

Some beliefs may be universal across systems (e.g. a prohibition on murder), but ethical practice may also vary between different systems, cultures and contexts. Ethical principles are sometimes in tension with each other, and different societies might agree different trade-offs and balances. For example they might vary in how they balance the rights of individuals, communities and society, or security and safety and individual privacy.

Negotiating this tension is important when choosing ethical principles for something with global application like web ML standards. Several considerations are important.

Firstly, in common with other fields, ethical ML principles generally operate at a high level of abstraction, allowing them to be more universal. For example, “fairness” is a common principle, but not a specific definition of fairness, leaving that to be negotiated within any particular socio-political context.

In choosing principles, we should take an approach which supports universality. One way to do this is by choosing principles which have evidence of global relevance and support, both through the process of developing them, and their subsequent adoption.

We can also foreground principles which empower users and support their agency and autonomy, so that they can make decisions for themselves, based on their own context and values.

Also, the W3C’s role is as a promoter of open standards, rather than specific technologies or implementations. While standards are not values-neutral, in practice some of the harder ethical questions (where specific values may vary) are more relevant to specific implementations or approaches than to the broader standards. This note offers principles and guidance for implementers and authors to make use of these standards according to their context.

# Appendix 2. Why the UNESCO principles were chosen  # {#appendix-unesco}

In response to ethical concerns and cases where ML has caused or contributed to harm, there has been an explosion in recent years of AI Ethical principles - there are now more than a hundred globally ([Linking Artificial Intelligence Principles](https://arxiv.org/abs/1812.04814) provides links to around 90 of them).

They have been developed by actors of all types, from trans-national bodies like the EU, OECD and UNESCO, to large companies, public-sector organizations, academia, private philanthropic concerns, and campaigning and activist groups. 

For the W3C Web ML working group, the first question is whether we should develop our own principles from scratch or adopt some already existing ones. Given the scope of the current remit, the resources required for proper stakeholder consultation and management around developing principles from scratch, and the existence of good candidates amongst already published principles, it is proposed to adopt and adapt existing principles. This does not preclude the development of more bespoke principles from scratch in the future.


## General considerations for choosing from existing ethical AI principles

Given the number of existing sets of principles, how should we choose amongst them? 

Some key criteria are:

- They should be as universal as possible, as evidenced by:
  - A diverse, global range of stakeholders involved in their development
  - Broad acceptance of the final result
- They should have good coverage (be as complete as possible, while not unnecessarily broad) as evidenced by:
  - Alignment with key principles found in meta-analyses of AI ethical principles, which investigate a number of sets of principles to look for convergence on common themes.
- They should align with relevant existing W3C principles and guidance in this space 

## Candidate universal ethical AI principles

For the following evaluations, see our [comparison of the various principles](#appendix-comparison).

Given the requirement for universality, the most likely source is transnational organizations, either governmental or non-governmental.

Some candidates:

- [[UNESCO]]
- [[EGTAI]]
- [OECD AI Principles](https://oecd.ai/en/ai-principles) / [G20 AI Principles](https://www.meti.go.jp/press/2019/06/20190610010/20190610010-1.pdf)

Of these, the UNESCO Recommendation stands out as the best candidate because:

- It is the product of an [inclusive, multi-disciplinary, global consultation and development process](https://en.unesco.org/artificial-intelligence/ethics#recommendation), as part of a global institution with non-Western participants (unlike EU)
- It has been [adopted by all 193 UNESCO member countries](https://news.un.org/en/story/2021/11/1106612) (NB: The US is not part of UNESCO and not a signatory of the new recommendations. But the UNESCO principles align with many developed in the US)
- It has a good breadth of principles (vs OECD/G20). Compared with the EU principles, it lacks an explicit statement about “Respect for Human Autonomy”. It could be argued that this is implicitly covered by the other values and principles, such as Value 1: **“**Respect, protection and promotion of human rights and fundamental freedoms and human dignity”. But while the other UNESCO Values are made more concrete by the principles that follow, Value 1 is less so, and hence there is less of an explicit commitment to Autonomy.
- Although UNESCO members are states, the principles are framed broadly enough that they can apply to anyone involved in the AI lifecycle, and the UNESCO guidance refers to ‘AI actors’ as well as states. 

## UNESCO Values and Principles

The UNESCO Recommendation consists of 4 values and 10 principles. According to the recommendation:

<blockquote>Values play a powerful role as motivating ideals in shaping policy measures and legal norms. While the set of values outlined below thus inspires desirable behaviour and represents the foundations of principles, the principles unpack the values underlying them more concretely so that the values can be more easily operationalized in policy statements and actions.</blockquote>

The UNESCO Values are:

- Respect, protection and promotion of human rights and fundamental freedoms and human dignity 
- Environment and ecosystem flourishing			
- Ensuring diversity and inclusiveness			
- Living in peaceful, just and interconnected societies	

The UNESCO Principles are: 

- Proportionality and Do No Harm
- Safety and security
- Fairness and non-discrimination
- Sustainability
- Right to Privacy, and Data Protection
- Human oversight and determination
- Transparency and explainability
- Responsibility and accountability
- Awareness and literacy
- Multi-stakeholder and adaptive governance and collaboration

## Sense checking UNESCO for universality

One of the main concerns in terms of universality is that many AI principles are developed from a Western perspective. So the UNESCO principles can be further sense-checked for against a set of non-Western country/region specific principles

- China [Governance Principles for the New Generation Artificial Intelligence--Developing Responsible Artificial Intelligence](http://www.chinadaily.com.cn/a/201906/17/WS5d07486ba3103dbf14328ab7.html)
- [Dubai AI Principles](https://www.digitaldubai.ae/initiatives/ai-prin)
- [Japan: Social Principles of Human-centric AI](https://www.cas.go.jp/jp/seisaku/jinkouchinou/pdf/humancentricai.pdf)

[Comparing these principles](#appendix-comparison) (UNESCO with the ones highlighted in orange), we see good general alignment across almost all the principles and values. This is truest for the principles at their highest-level, simplest formulations. The more detailed explanation of some principles reveals particular countries’ more specific policy concerns and emphases which occasionally diverge. This is not a major concern, but suggests we should exercise some caution to avoid being over-specific in fleshing out the principles.

UNESCO also continues to actively consider issues of universality and cultural diversity, and has a number of useful resources including this video on [Shaping AI through Cultural Diversity](https://www.youtube.com/watch?v=AiK0iYZuNS0).


## Sense checking UNESCO for appropriate coverage

To check for completeness and appropriate breadth, we can compare with meta-analyses of AI Principles. There are a number of these, often referring to each other, but the following two provide a good level of coverage and depth. 

- [Principled Artificial Intelligence Mapping Consensus in Ethical and Rights - based Approaches to Principles for AI](https://dash.harvard.edu/bitstream/handle/1/42160420/HLS%20White%20Paper%20Final_v3.pdf?sequence=1&isAllowed=y) by Harvard Berkman Klein Centre
- [The Ethics of AI: Evaluation of Guidelines](https://link.springer.com/content/pdf/10.1007/s11023-020-09517-8.pdf) by Thilo Hagendorf

Here again [we can see](#appendix-comparison) (comparing UNESCO with the ones highlighted in pink) that there is good alignment of the UNESCO principles with the most popular principles that emerge from the meta-analyses. UNESCO covers them all, but is not going too far beyond them - the main differences are that “awareness and literacy” is not among the top principles in either meta-analysis, and “sustainability” does not appear in the Harvard one. 

<div class="advisement" heading="RECOMMENDATION">Given the universality and appropriate coverage of the UNESCO recommendation, it is proposed to adopt the principles and values as the basis for this document.</div>

## Alignment with W3C principles and values

The final consideration is how the UNESCO principles align with various relevant W3C statements of principles, values and areas of interest, and whether they need to be augmented at all. 

### Analysis of key themes from W3C documents

There are a number of different places we can look for guidance on W3C values which are relevant to ethical ML. They range from established vision to non-normative works in progress, and are reviewed in roughly that order:

<dl>
<dt>[W3C Vision, Mission and Design Principles](https://www.w3.org/Consortium/mission)
<dd><p>Relevant for consideration here is W3C’s vision of “One web”, the focus on “web for all” including accessibility and internationalization, and “web of trust”, including security, privacy and trust more broadly. 
<p>These all map well to the UNESCO Recommendation. Trust is both enhanced by specific principles such as privacy and security, but also as the supporting text of the UNESCO Recommendation points out, by the effective operationalisation of the Recommendation as a whole. 
<dt>[W3C Horizontal Review Working/Interest Groups](https://www.w3.org/Guide/documentreview/#how_to_get_horizontal_review)
<dd><p>Relevant here are the areas of activity of the horizontal review groups, as this is an indication that W3C places a high priority on the values they represent. The key ones are: accessibility, internationalization, privacy and security. 
<p>As above, these all map well to the UNESCO Recommendation. 
<dt>[W3C TAG Ethical Web Principles](https://w3ctag.github.io/ethical-web-principles/#principles)
<dd><p>Architecture is not considered relevant above as an ethical value in itself, but clearly the output of the TAG, especially their Ethical Web Principles, has strong relevance. This is a non-normative document, representing the consensus view of TAG around principles to guide their work, and that of others. 
<p>[Mapping these principles against the UNESCO recommendations](#appendix-comparison), we can see a good level of overlap. There are two that map less well (The web is multi-browser, multi-OS and multi-device; People should be able to render web content as they want) but this is because of their quite specific technical focus on consumption of the web, so is not considered a problem.
<p>Less clear is “the web must enhance individuals control and power” - which seems most aligned with a principle of autonomy, but as noted above the UNESCO recommendation has only an implicit focus on that, with no explicit statement of it as a principle. 
<dt>[Web platform design principles](https://www.w3.org/TR/design-principles/)
<dd><p>This is another TAG document which builds on the Ethical Web Principles. Relevant for consideration here are the principles in section 1: put user needs first; safety (including security and privacy and informed decision-making); trust; and meaningful consent. 
<p>Again these mostly map well to the UNESCO recommendation, with a similar note as above that informed decision-making and consent might sit most comfortably with an explicit principle of user autonomy, but can be accommodated within the other principles. 
<dt>[A New Focus for the W3C: Improving the Web’s Integrity](https://github.com/WebStandardsFuture/Vision)
<dd><p>According to this document, it is “intended to be a stronger vision statement for the W3C. This is currently exposed as a work item of the W3C Advisory Board, on the AB wiki” and “builds on the basis of the Technical Architecture Group's excellent Ethical Web Principles.”
<p>The values and principles include many of the themes above (accessibility, security, trust), as well as some additional ones which echo the central concerns of ethical ML (transparency, equity, fairness). They generally map well to the UNESCO Recommendation. 
<p>UNESCO’s principle of “Multi-stakeholder and adaptive governance and collaboration” also aligns well with the articulation of the W3C’s purpose and identity. 
<p>The area which perhaps maps least directly is W3C’s concern with an interoperable, de-centralized web.
</dl>

### Summary of W3C alignment

From the above, we can see that there is generally good alignment between the UNESCO recommendation, and the values and principles which W3C has expressed in various places. There is certainly no conflict where UNESCO is proposing anything counter to W3C’s values. 

As noted, there are some areas where values expressed by W3C are not articulated directly as UNESCO principles, mainly things related to “Autonomy” and the concern with an interoperable, de-centralized web. The lack of Autonomy as a principle was also noted earlier in comparison with the EU Principles.

Given this …

<div class="advisement" heading="RECOMMENDATION">It is proposed to supplement the UNESCO principles with an additional principle of “Autonomy”, to make more concrete the commitment in Value 1 to “Respect, protection and promotion of human rights and fundamental freedoms and human dignity”</div>

# Appendix 3. Comparison of Ethics Principles # {#appendix-comparison}

See: [spreadsheet](https://docs.google.com/spreadsheets/d/1hzOHVYlC4OfE2-UE6942LnYEgP_kJmyF3u4JJwwcNpA/edit?usp=sharing)
